---
title: "Classification 2"
output: html_notebook
---

# Classification 2

```{r}
library(ISLR)
attach(Default)
summary(Default)
```

## Performing KNN classification in R

#### to perfrom KNN classificatino, we use `knn()` in the `class` package.

```{r}
install.packages("class")
library(class)
```
- R will randomily break the nearest neighbors tie.
- enable reproducibility: set a seed before running `knn()`

```{r}
set.seed(1)
```

## knn()requires four inputs:
1) Matrix of feature measurements for training data
2) Matrix of feature measurements for test data
3) Vector of class labels for training data
4) Value for ğ¾, the number of nearest neighbors to use

### Input 1: Matrix of feature measurements for training data
 
- We will use KNN to make predictions about default from student, balance, and income.
- Recall that we have 10,000 observations on four variables: default, student, balance, and income.
- We need to split this dataset into a training dataset and test dataset.
- Letâ€™s use the first 9,000 observations (rows) as our training dataset:
```{r}
train = seq(1,9000)
train.x = cbind(student, balance, income)[train,]
test = seq(9001, 10000)
test.x = cbind(student, balance, income)[test,]
train.y = default[train]
knn.pred = knn(train.x, test.x, train.y, k=1)
summary(knn.pred) #summary
knn.pred #full output
```

## Assessing test accuracy of KNN in predicting default in R

- KNN classification with K=1

```{r}
knn.pred = knn(train.x, test.x, train.y, k=1)
test.y = default[test]
```

#### creating a table 

```{r}
table(knn.pred, test.y)
```

## Assessing test accuracy of KNN in predicting default in R

- Test accuracy rate is just the total number of correct predictions $(y_0 + y_0(hat)) $  Divided by the total number of predictions:

```{r}
# test error rate
(943 + 10) / 1000

# can also use the mean() function
mean(knn.pred == test.y)
```

#### finished with stuff for HW 3

## Simple Logistic Regression
- Simple logistic regression models the probability that ğ‘Œ belongs to a particular class ğ‘— given a single feature ğ‘‹ with the logistic function
- see slide 106 in lecture 8, classification, to see the formula 

## stopped at 2:36:25

