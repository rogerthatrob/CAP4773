---
title: "Classification"
output: html_notebook
---

# Classification 2

```{r}
library(ISLR)
attach(Default)
summary(Default)
```

## Performing KNN classification in R

#### to perfrom KNN classificatino, we use `knn()` in the `class` package.

```{r}
install.packages("class")
library(class)
```
- R will randomily break the nearest neighbors tie.
- enable reproducibility: set a seed before running `knn()`

```{r}
set.seed(1)
```

## knn()requires four inputs:
1) Matrix of feature measurements for training data
2) Matrix of feature measurements for test data
3) Vector of class labels for training data
4) Value for ğ¾, the number of nearest neighbors to use

### Input 1: Matrix of feature measurements for training data
 
- We will use KNN to make predictions about default from student, balance, and income.
- Recall that we have 10,000 observations on four variables: default, student, balance, and income.
- We need to split this dataset into a training dataset and test dataset.
- Letâ€™s use the first 9,000 observations (rows) as our training dataset:
```{r}
train = seq(1,9000)
train.x = cbind(student, balance, income)[train,]
test = seq(9001, 10000)
test.x = cbind(student, balance, income)[test,]
train.y = default[train]
knn.pred = knn(train.x, test.x, train.y, k=1)
summary(knn.pred) #summary
knn.pred #full output
```

## Assessing test accuracy of KNN in predicting default in R

- KNN classification with K=1

```{r}
knn.pred = knn(train.x, test.x, train.y, k=1)
test.y = default[test]
```

#### creating a table 

```{r}
table(knn.pred, test.y)
```

## Assessing test accuracy of KNN in predicting default in R

- Test accuracy rate is just the total number of correct predictions $(y_0 + y_0(hat)) $  Divided by the total number of predictions:

```{r}
# test error rate
(943 + 10) / 1000

# can also use the mean() function
mean(knn.pred == test.y)
```

#### finished with stuff for HW 3

## Simple Logistic Regression
- Simple logistic regression models the probability that ğ‘Œ belongs to a particular class ğ‘— given a single feature ğ‘‹ with the logistic function
- see slide 106 in lecture 8, classification, to see the formula 

# Generalized Linear Models

## The `glm()` function

- The `glm()` function fits generalized linear models, a class of models that includes logistic regression. 
- syntax: `glm(formula, data, family, ...)`
- supports `attach()`
- `family` tells `glm()` which model to use, ie: `family = binomial` 

```{r}
glm.fit = glm(formula = default ~ balance, family = binomial)
summary(glm.fit)
```

- Estimate is beta. The intercept is Beta_not_hat and the balance (or the first feature) is Beta_one_hat
-- in this case Beta_not_hat = -10.65 and Beta_one_hat = 0.0055
- Std. Error is again the average distance between Beta_hat and Beta. There is a small difference between Beta_hat and Beta meaning our estimation is pretty good. 
- z value is our test statistic for gml(). test statistic z to evaluate H_not: Beta_i = 0.
-- will tell us how far away our estimate is from what we expect from the null hypothesis. 
- P value for H_not: B_i = 0, want small to reject the null hypothesis 
- Null deviance and the Residual deviance measure the fit of the model. are the predicted vs the observation without null and with B_1 (residual).
- **Deviance** is analogous to RSE in linear regression - it measures lack of fit of model (predictions) to data (observations)
- **Null deviance** is lack of fit of the null model (B_not only)
- **Residual Deviance** is lack of fit of the alternative model (B_not and B_1)
- in the above example the null deviance = 2920.6 and residual deviance = 1596.5
- **Akaike Information Criterion** (AIC) is analogous to the adjusted R^2 in linear regression - it measures goodness of fit adjusted for model complexity.
- AIC rewards goodness of fit (likelihood to parameters), but penalized as function of number of estimated parameters in the model. 
- The smaller the AIC, the better the model fits the data. 
- But in contrast to R^2, the value of AIC itself is not meaningful, only its comparison to those of other models. It is a measure of fit like the null and residual deviance. 
- from the above example, AIC = 1600.5
- Number of Fisher Scoring Iterations, less than 20 is good.

## Obtaining confidence intervals for B_0 and B_1 in R
- can us the `confit()` function to obtain confidence intervals for parameters of out logistic regression function.

```{r}
confint(glm.fit)
```

## Making predictins about `default` and `balance` in R
- As in linear regression, we can use the `predict()` function to make predictions from our logistics regression function. 
- the only difference is that we need to set the argument `type = "response"` to output a probability rather than the *logit*.
- to predict the probability of `default` given a `balance` of 1000 dollars or 2000 dollars: 

```{r}
predict(glm.fit, data.frame(balance = c(1000, 2000)), type = "response")
# this will avoid the rounding errors
```

## Getting class labels from logistic regression in R
- first, need to obtain each probability $\hat{P}(default = Yes|balance)$
- to do this, we need the `predict()` function:
```{r}
glm.probs = predict(glm.fit, type = "response")
# glm.probs
```
- next, we need to convert probabilities to classes by setting a threshold.
- use `rep()` to create a vector of 10,000 "No" values:
```{r}
glm.pred = rep("No", 10000)
```
- then we change values to "Yes" when $\hat{P}(default = Yes|balance) > 0.5$:
```{r}
glm.pred[glm.probs > 0.5] = "Yes"
```
- we can use the `table()` function to view predicted vs. true classes: 
```{r}
table(glm.pred, default)
```
- we can calculate accuracy create either of the following: 
```{r}
# calculate it manualy
(9625 + 100) / 10000

# calculate it using the mean function
mean(glm.pred == default)
```
- this means 97.24% of our results are accurate. 
- it does not mean the test accuracy is this correct. 

## Need to seperate training and test data to asses test accuracy. 

## Re-fitting a logistic model to `default` training data in R

```{r}
train = seq(1,9000)
test = seq(9001, 10000)
```
- to fit a logistic regression model only to the training data, we can use the `subset` argument in `glm()`
```{r}
glm.fit = glm(default ~ balance, family = binomial, subset = train)
summary(glm.fit)
```

## Obataining confidence intervals for ${\beta}_0$ and ${\beta}_1$ in R

- we can obtain 95% confidence intervals for ${\beta}_0$ and ${\beta}_1$:
```{r}
confint(glm.fit)
```

## Making Predictions about test data in R
- we can now predict the probabilities of `default` for all 1000 values of `balance` in our test data: 
```{r}
glm.probs = predict(glm.fit, Default[test, ], type = "response")
```

## Review: Getting class labels from logistic regression in R
- Recall that we need to convert probabilities to classes.
- For example,if $\hat{P}(default=Yes|balance) >0.5$,then set default="Yes".
- Again, we first use `rep()` to create a vector of 1000 â€œNoâ€ values:
```{r}
glm.pred <- rep("No", 1000)
```
- Then we change values to â€œYesâ€ when $\hat{P}(default=Yes|balance) >0.5$:
```{r}
glm.pred[glm.probs > 0.5] = "Yes"
```

## Assessing test accuracy in predicting default from balance
- We can again use `table()` to view predicted vs. true classes: 
```{r}
table(glm.pred, default[test])
# also calculate the test accuracy rate
(960+9) / 1000
mean(glm.pred == default[test ])
```



